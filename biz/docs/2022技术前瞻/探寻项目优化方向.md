[TOC]

# 痛点
- 启动本地服务慢
- npm依赖系统混杂
- 打包慢
- 不安全【没做好权限控制，使得非顶层依赖也可以直接使用】
- 依赖包管理控制

# `pnpm`
## 速度
总结速度对比就是：pnpm > yarn > yarn PnP > npm

> 注：在存在缓存的情况下，`yarn PnP`是速度最快的

![pnpm下载速度对比](https://camo.githubusercontent.com/83b108abddef5c40f6afc985fa8214edc92b6f2226a83d577074a720907463c8/68747470733a2f2f706e706d2e696f2f696d672f62656e63686d61726b732f616c6f7474612d66696c65732e737667)

这是`pnpm`的优势之一：速度是同类工具中最快的。

## 依赖
以下我会进行一组对比。

首先是`vite-react-demo`，这是我利用`vite`创建的模板项目，创建方式不重要。

我们看一下`package.json`：
```json5
{
  //其他属性
  "dependencies": {
    "react": "^17.0.2",
    "react-dom": "^17.0.2"
  },
  "devDependencies": {
    "@types/react": "^17.0.33",
    "@types/react-dom": "^17.0.10",
    "@vitejs/plugin-react": "^1.0.7",
    "typescript": "^4.4.4",
    "vite": "^2.7.2"
  }
}
```
可以看到共有7个首层依赖。

这是我们使用`pnpm`安装的结果：
![pnpm](https://pic.imgdb.cn/item/61f36fa22ab3f51d91afe9a4.jpg)

可以看到除了`.bin`和`.pnpm`两个公共目录，其他都是**首层依赖包**，非常清爽和干净。

这是使用`yarn`安装的结果：
![yarn](https://pic.imgdb.cn/item/61f370612ab3f51d91b096db.jpg)

`otcd`项目我也进行了同样的测试，结果也是类似的，使用`pnpm`的`node_modules`目录基本只有首层依赖，非常干净。

简单说一下为什么`pnpm`的下载结果只有首层依赖？

1. 在`npm 3`版本之前`npm`都是将首层依赖的依赖包安装在对应目录下，但是这种做法缺陷很明显，会造成重复下载依赖
2. 在`npm 3`开始处理这个问题，`npm`会对比所有需要的依赖包，有一个包的兼容范围，比如说假设`lodash`的`2.0.0`到`2.3.9`范围是完全兼容的，然后我们首层依赖多层使用了`lodash`这个包，那么`npm`只会下载兼容范围内最新的包，以此减少重复下载
3. 因而子层级的包也被放在了首层目录中，造成首层目录的臃肿。
4. 而`pnpm`则是将首层依赖所依赖的包放在`.pnpm`目录下，这样就可以在避免重复下载的同时，保证首层目录的干净

为什么要保证首层依赖的干净？
1. 方便开发者在需要的时候查找相关顶层依赖代码
2. 权限严格：原本node_modules的扁平结构使得非顶层`npm`包也可以直接访问，比如实际项目中我遇到过有人在没有指定顶层依赖的情况下直接使用软件包，比如`react-router`和`queryString`

除了速度和目录方面的优势，在内存占用上`pnpm`也有极大的优势，这里我们先介绍下`yarn PnP`

## 关于`PnP`特性【即插即用】
`PnP`是facebook针对`yarn`的痛点给出的解决方案，首先是在`facebook`内部测试使用，2018年9月公开。

> 注：yarn PnP特性必须将yarn升级到2以上才能升级，facebook考虑到兼容问题，默认安装yarn都是1.x版本，升级2以上需要手动执行命令`yarn set version berry`

### 解决问题
直接原因是为了解决`yarn`的两个痛点：
1. 引用依赖慢
2. 安装依赖慢

首先看一下引用依赖时的问题。

node在处理依赖引用时，会有以下两种情况【这里对内部模块解析只是简化说明】：
1. 如果我们传给`require()`调用的参数是一个核心模块（比如`fs`、`path`等）或者是一个本地相对路径（比如`./views/v1`），那么node会直接使用对应的文件
2. 如果不是前面描述的情况，那么Node会开始寻找一个叫做`node_modules`的目录
   1. 首先`Node`会在当前目录寻找`node_modules`，如果没有则到父目录查找，以此类推到系统根目录
   2. 找到`node_modules`目录之后，再在该目录中寻找名为`moduleName.js`的文件或者是名为`moduleName`的子目录

所以`Node`在解析依赖是需要进行大量的文件I/O操作，效率不高。

在看一下在安装依赖时的情况，执行`yarn`会执行以下4个步骤：
1. 将依赖包的版本区间解析为某个具体的版本号
2. 下载对应版本依赖的 tar 包到本地离线镜像
3. 将依赖从离线镜像解压到本地缓存
4. 将依赖从缓存拷贝到当前目录的 node_modules 目录

主要是第4步，涉及到大量I/O操作，导致安装效率不高（尤其是CI环境，每次都需要安装全部依赖）

### 实现原理
facebook工程师针对这些问题决定给出一个可以彻底解决问题并且可以和现有生态进行兼容的解决方案，这个方案就是`PnP`。

说一下PnP的具体工作原理，作为将依赖从缓存拷贝到`node_modules`的替代方案，Yarn会维护一张静态映射表，该表包含了以下信息：
1. 当前依赖树中包含了哪些依赖包的哪些版本
2. 这些依赖包是如何相互关联的
3. 这些依赖包在文件中的具体文职。

这个映射表对应的就是目录中的`.pnp.js`文件

这个`.pnp.js`是如何生成且使用的呢？

在安装依赖第4步时，Yarn不会拷贝依赖到`node_modules`目录，而是会在`.pnp.js`中记录下该依赖在缓存中的具体位置，这样就避免了大量的I/O操作同时项目目录也不会有`node_modules`目录生成。

同时，`.pnp.js`还包含了一个特殊的`resolver`，Yarn会利用这个特殊的`resolver`来处理`require`请求，该`resolver`会根据`.pnp.js`文件中包含的静态表直接确定依赖在文件中的具体位置，从而避免了现有实现在处理依赖引用时的I/O操作

这里我们总结下`PnP`的好处：
1. 安装依赖速度空前提升
2. `CI`环境中的多个CI实例可以使用同一份缓存
3. 同一个系统中的多个项目不再需要占用多分磁盘空间

### 使用
首先要开启`pnp`特性
1. `cra`已经做了集成，只需要在项目中添加`--use-pnp`即可
2. 已有项目执行`yarn --pnp`即可开启

在`package.json`配置：
```json
{
   "installConfig": {
      "pnp": true
   }
}
```

由此可见，`Plug'n'Play`【即插即用】这个特性对我们来说是很有意义的。

现在`pnpm`也支持`pnp`特性，这是使用`pnp`模式下载依赖的`node_modules`结果：

![](https://pic.imgdb.cn/item/61f3a1752ab3f51d91e0f1eb.jpg)

然后是内存方面的对比，这是`yarn`下载的：

![](https://pic.imgdb.cn/item/61f3a2942ab3f51d91e27008.jpg)

这是`pnpm`普通模式下载：

![](https://pic.imgdb.cn/item/61f3a2582ab3f51d91e20956.jpg)

这是`pnpm`使用`pnp`特性下载：

![](https://pic.imgdb.cn/item/61f3a2432ab3f51d91e1f013.jpg)

## 项目引入
目前来说，`pnpm`是当前前端最优秀最先进的技术方案，解决了`npm`的很多问题。

我是想在`2022`的项目中进行使用，这会使得现在混乱的包管理变得清晰很多，而且速度更快，更安全。

不过有一些东西需要考虑下：
1. 因为`yarn`和`npm`没有做好权限控制，导致我们项目存在非顶层依赖被直接使用的情况，所以如果直接换成`pnpm`会出现`resolve not found`的情况，不过这个本身就是一个安全风险的隐患，应该及早解决掉。我已经在`otcd`项目测试过来，主要就是`react-router`这个包被滥用，这个包是`react-router-dom`的依赖包，解决方案也很简单，将其替换为`react-router-dom`即可
2. 关于`pnp`模式，是使用简单模式，还是启用`pnp`模式
3. 关于`Yarn2`，`Yarn2`也是一个很优秀的方案，无论速度还是包管理都更进一步，如果我们不使用`pnpm`，则可以使用`yarn2`或`yarn3`作为候补选择。

# `esbuild`
打包工具目前主流有：`esbuild`、`swc`、`babel`这几种

从速度上来看：`esbuild` ≈ `swc` >> `bebel`

![](https://github.com/evanw/esbuild/raw/master/images/benchmark.svg)

esbulid和swc几乎都是babel的几十倍速度，不过我个人推荐使用`esbulid`，原因在于：
1. esbuild的开发者是`Figma`【一个向量图形编辑器和原型设计工具】的`CTO`，而`swc`者是一个97年的年轻人，他写`swc`的时候还只是大二。
2. 另一方面来说，很多成熟的工具，比如`vite`、`umi`选择了`esbulid`作为打包工具
3. github关注数来看，esbulid是30.9k，而swc是19.9k，相差50%

`esbuild`和`swc`之所以比`bebel`快这么多，主要是开发语言上的差距，`bebel`本身是非常优秀的，可惜它是使用`js`开发的，`js`是单线程、解释性语言，相比较多线程、编译型语言性能上差距很大，而`esbuild`是使用`go`开发的，`swc`是使用`rust`开发的。

使用`esbuild`将有效提升我们的启动速度和打包速度。

## 项目使用
实际业务项目很少说独立使用`esbuild`在项目中的使用，通常是集成到`webpack`配合打包。

我们团队项目都是使用`cra`进行创建的，`webpack`配置被隐藏，这里我使用了`react-app-rewired`进行配置更新。

我们看一下要怎么做：
1. 安装`esbuild-loader`
2. 移除`babel-loader`、`ts-loader`，替换为`esbuild-loader`
3. 添加插件`ESBuildPlugin`

下面是我的进行项目实践的配置：
> speed-measure-webpack-plugin用于测试速度

```js
const SpeedMeasurePlugin = require("speed-measure-webpack-plugin");
const {ESBuildPlugin} = require('esbuild-loader')

const swp = new SpeedMeasurePlugin();

module.exports = function (config,dev) {
    config = swp.wrap(config)
    config.plugins.push(new ESBuildPlugin());
    const rules = config.module.rules[2].oneOf;
   //使用esbuild-loader替换babel-loader
    rules.splice(2,1,{
        test:/\.(js|mjs)$/,
        loader:'esbuild-loader',
        options:{
            loader:"jsx",
            target:"es2015"
        }
    })
    return config;
}
```

`esbuild`也提供了压缩插件，不是这个不是我们关注的重点，不进行深入，有需要可以使用。

使用`esbuild`也有助于提升启动服务速度，不过相对来说我们并不十分在意这一点，使用`esbuild`大约可以提升`20%-80%`的启动速度，这点速度和`webpack`提供的缓存带来的性能提升差不多，不能说提升太少，只是我们有更强大的构建方案，真正给力的构建优化方案的可以带来数十倍上百倍的性能提升，可以做到毫秒级、秒级的启动。

而且`esbuild`的构建提升和项目大小依旧相关，它只能按比例提升速度，项目越大速度阅读，所以，我们并不将`esbuild`作为解决服务启动慢的主要方法。当然，`esbuild`对启动服务是有显著提升的。

接下来我会介绍两种主流方案，这两种方案不会因为项目大小而显著降低速度，可以认为项目越大则使用这两种方案带来的效率提升越大，因为启动速度几乎是不变的。

# `esm`
`esm`是一种模块加载方案，相比于`node`的`commonjs`方案，逐渐成为新的主流方案，其原因在于其优秀的加载速度。

简单说一下为什么`esm`相比`commonjs`的加载有着这么巨大的速度差异，简单来说就是“按需加载”。

比如说现在我们项目有数十个页面，数百个文件。

那么按以往的`commonjs`模式需要使用`bebel`处理所有的项目文件然后进行代码加载，这种模式项目越大服务启动速度越慢。

目前使用`commonjs`的模块加载是全量加载，这种加载方式负担很大，因为需要将所有相关文件都进行加载，如下图：

![commonjs加载](https://pic.imgdb.cn/item/61ff91b42ab3f51d91af431c.jpg)

而`esm`则是按需加载，可以简单视作按路由将相关文件进行加载，比如说我想展示`/view/v1`路由的页面，那么只会加载与这个路由相关的文件，正因如此，项目的大小不会影响服务启动的速度。 如下图：

![esm加载](https://pic.imgdb.cn/item/61ff93022ab3f51d91b09225.jpg)

目前我知道的使用`esm`的主流工具有`vite`和`snowpack`两种，下面我说一下这两种工具。

## `snowpack`
去年（2021）`vite`大火，然而其实早在2020年就有人先一步进行了探索，那就是`snowpack`，它是一种新的打包工具。

在2020年那个时候snowpack就已经率先应用`esm`了，不过可惜往往真正的并不是开拓者，因为当时浏览器支持和生态环境尚且荒凉，snowpack并没有爆火，2021年`esm`的生态和使用环境日趋成熟，加之`vite`本身的优秀，成功在`2021`年爆火，打破了`webpack`一家独大的场景。

截至2022.2.4日，vite关注数37.1k，snowpack关注数19.7k,两个已经有了明显的差距。

snowpack是非常强大的打包工具，它开创了一个新的前端开发风格——`bundleless`

`bundleless`简单理解就是“无打包”或者“去打包”

以最常见的打包工具webpack为例，我们本地开发服务需要进行打包，将高级语法特性转换成`es3`语法版本并整合到`bundle`中统一引用。

简而言之，snowpack最大的特点就是快，无论是全量更新还是增量更新都快的一批，它为什么这么快？快慢都是相对的，或许说webpack慢更合理，因为webpack目前的打包方式需要构建一种完整的依赖图，然而据此合并计算得到一个完整的打包。

因为这种打包模式的一大痛点就是“外部“依赖的处理，“外部”依赖是指：
1. 模块 A 运行时对 B 是有依赖关系
2. 但是不希望在 A 构建阶段把 B 也拿来一起构建

举一个我自己的亲身实例来说，我以前使用`rollup`开发`react-keep-router`库的时候就遇到过这个问题，我使用了`react`，它打包的时候会将`react`构建进去，一下子就多了几万行代码，如果使用这种方式那构建的包会非常大，很不合理。

解决方案是使用`external`，表示这个包是外部依赖，并不需要在构建的时候获取相应模块，而只是在运行时才有耦合关系。
【原生的JavaScript Module天生就是构建时非依赖，运行时依赖的。】

webpack之前也是一样的解决方案。不过在`webpack5`中提供了`module-federation`作为解决方案，简答来说，会区分远程模块和本地模块，构建打包是不获取远程模块，远程模块将会通过异步请求获取。这个模式也有利于微前端的发展。

[module-federation文档](https://webpack.docschina.org/concepts/module-federation/)

事实上，不仅是启动，在热更新的速度上`snowpack`也是爆杀webpack，根据实例测试，同一项目下，webpack热更新速度是3-5秒，而snowpack的速度是几十毫秒到一百毫秒。

相比webpack，snowpack根本不需要关注其他文件，它不需要进行复杂的计算和合并，模块之间是“松散”耦合

在`webpack`诞生之初他的打包做法是很有意义的，其历史原因在于：
1. 主流浏览器对于es6等最新特性支持不足
2. 对`import/export`模式支持不足，需要在`bundle`中通过闭包实现模块化
3. 在HTTP1.1版本中，并发请求会带来性能问题，合并代码通过一个请求获取bundle可以有效提升性能

为什么现在`vite`和`snowpack`这种`bundleless`风格的打包工具开始火起来了呢？
1. 主流浏览器对es6等新特性支持度足够高
2. import/export已经被全面支持
3. HTTP2.0版本已经普及，并发请求不再是性能问题

> 这里补充两张对比图...

这里稍微提一句，有人可能认为自己开发是使用最新版浏览器，但是无法保证用户的浏览器版本，可能会导致问题。

我们的答案是并不会，因为这个是解决本地服务启动和更新的，发给用户的版本完全可以进行打包，所以从两个方面来看：
1. 本地启动服务：我们开发者本身肯定是可以控制自己使用最新版浏览器的，在此前提下使用`esm`和`bundleless`，我们可以享受到：
   1. 省略打包步骤
   2. 并发请求
   3. 按需加载
2. 发给用户的版本：通过`esbulid`打包，保证兼容性

其实这个问题snowpack也早有考虑，它有提供专门的插件，如果有需要使用插件打包即可。【比如@snowpack/plugin-webpack / @snowpack/plugin-parcel】

## `vite`
`vite`是`vue`作者尤雨溪开发的构建工具。

vite很有意义一点在于它整合了很多前端好用的工具链：
1. 本地服务启动和热更新：`esm`
2. 构建指令配置：`rollup`【我用过rollup开发过一些项目，真的很好用，好就好在它入门门槛低，没有webpack那么复杂，我自己感觉麻烦的是选择相关的插件，虽然官方提供了一些推荐，不过这个还是得自己选择和测试，这个比较麻烦，语法和配置本身很简单，关键还是生态。vite进一步降低了配置门槛，它进行了预配置，有大佬们帮我们测试和组合，就不需要自己组合和选择了，省了很多事，有需要可能再增加或修改配置】
3. 打包工具：`esbuild`

这样一看，除了包管理器，`vite`几乎解决我们项目存在的所有痛点：
1. 启动速度和热更新慢：esm
2. 打包速度慢：esbuild
3. 配置复杂：预配置和rollup

vite构建工具链完整，这意味着如果我们从`webpack`迁移成本的降低。

目前稍微麻烦的是，`vite`对vue生态支持更加友好，对于react的生态可能有些插件没有对应的vite版本，这一点是需要考虑的。

# `webpack5`物理缓存
`webpack5`之前官方没有特别好的构建优化方案，因而社区倒是诞生了非常多的解决方案，这是我以前搜集的一些项目构建优化方案【比如说loader缓存、多进程、CDN这些】：

[项目打包优化2020](http://note.youdao.com/noteshare?id=8f24e1a299295760443e9a4b5c05d744&sub=991826D323684468B233BBC9BB6FD186)

`webpack5`官方推出了“物理缓存”/“持久化缓存方案”，构建效果有多强呢？

**官方经过测试，16000 个模块组成的单页应用，速度竟然可以提高 98%！其中值得注意的是持久缓存会将缓存存储到磁盘。**

就是说，大型项目的提升足有**50倍**的提升！

除了第一次是全量更新，后续启动模式是：读取缓存 -> 模块校验 -> 解封模块内容

无缓存的启动我们称为之**冷启动**，有缓存的启动我们称之为**热启动**，热启动的速度相比热更新非常相近。【略慢于热更新】

在物理缓存方案出现之前，webpack构建速度是真的被`esm`方案吊着打，差距太大了，使用物理缓存方案不说比得上`esm`，也可以极大提升构建速度，使其达到一个可接受的范围内。

我已经在`otcd`项目本地进行实践了，目前来看基本都在5秒内启动【第一次启动没有缓存，会比较慢】，体验还可以。至少比之前一分多钟两分钟来的好的多。

## 使用
通过配置开启即可【需要升级到webpack5】
```js
{
    cache: {
       type: 'filesystem'
    }
}
```
仅仅配置`type`，缓存会被固定，根据情况可能导致以下问题：
1. `mode`之类的变化无法响应，缓存不会变。
2. 如果根据不同的场景，有不同的`babel`配置等，也同样不会感知，依然会用旧的缓存。

[//]: # (3. 使用`DefinePlugin`注入的动态内容，全部不会变化。)

简而言之，我们需要动态相应配置变化。

通过`version`即可，如下：
```js
module.exports = {
  //...
  cache: {
    type: 'filesystem',
    version: 'your_version',
  },
};
```

刚刚这个是官方的示例写法，如果实际项目使用，则需要自己封装下version的生成，比如：

```ts
const computeCacheKey = (entry: BuildEntry): string => {
    const hash = crypto.createHash('sha1');
    hash.update(entry.usage); // 使用场景，如build、dev等
    hash.update(entry.mode);
    hash.update(entry.hostPackageName); // 包名
    hash.update(fs.readFileSync(path.join(entry.cwd, 'settings.js'))); // 定制化配置
    hash.update(fs.readFileSync(path.join(entry.cwd, 'node_modules', '.yarn-integrity'))); // 依赖信息,与包管理工具相关
    return hash.digest('hex');
};
```

更详细的配置：[缓存配置文档](https://webpack.docschina.org/configuration/cache)

## 缓存原理简述
任何一种缓存体系，有两点至关重要，必须要考虑：
1. 缓存校验
2. 缓存容量限制

简单说一下，首先是缓存校验，webpack5之前使用的是`timestamps`【时间戳】校验，并不可靠，而且效果并不是很好，理由：
1. 时间戳变化，但是内容没有变化，此时会进行缓存更新【然而我们知道不更新更合理】
2. 时间戳没变化，但是内容变化了【比如说文件重命名】

另外基于文件系统`metadata`的比较算法也不可靠，因为`metadata`是通过修改时间和文件大小混淆得到的

`webpack5`选择的缓存校验算法是基于文件内容的`hash`算法，毫无疑问更加可靠，并且可以避免不必要的更新。

再说一下缓存容量限制，刚刚我们看到配置中`version`的存在，这意味着随着动态变化，会出现许多份缓存，所以对于磁盘容量的控制和缓存清理是不可缺少的关键部分。

目前`webpack5`的清理规则如下：
1. 对于1个缓存集合，不超过5个缓存内容
2. 最大积累资源不超过500MB
3. 有效时长2周

超过阈值时会优先删除最旧的缓存内容。

这是一个经典的**LRU**缓存算法，比如手机APP清理、路由缓存清理使用的都是这个淘汰算法，其核心思想就是：“如果数据最近被访问过，那么将来被访问的几率也更高”

react官方就有实现这个方案，我以前在 [数据管理优化](http://note.youdao.com/noteshare?id=9e934c47b45c951317161a1c354e27e4&sub=EA6B56DB5F8046AD87E881FBE1B8D6FB) 的“复用数据”的“react-cache”部分有详细解读过源码，一般我们通过 哈希表 和 双向链表 实现LRU。

实际上otcd项目就有一个我实现的LRU，用于管理路由缓存。

对LRU算法感兴趣推荐学习react官方的实现，我看的是`17.0.2`版本源码，在`packages/react-cache/src/LRU.js`

## 迁移
如果要使用物理缓存，则必须先升级`webpack5`，在`2022`年我们可能要花几天进行升级，迁移涉及：
1. webpack本身的配置变化
2. 相关loader变化
3. 相关plugin变化

实际上可能比我之前想象的要简单些，我们项目是通过`create-react-app`创建的，而`cra`基本不需要升级，因为它将webpack几乎全权委托给`react-scripts`。

然后我查了`react-scripts`的升级文档，有一个好消息：**react-scripts 2021.12.14号发布的5.0.0版本支持webpack5啦！**

更新日志：[create-react-app升级文档](https://github.com/facebook/create-react-app/blob/main/CHANGELOG.md)

这意味这我们只需要进行以下几步：
1. 升级react-scripts至5.0.0
2. 通过`react-app-rewired`处理自定义配置的兼容

升级指令（无需reject）：
```
npm install --save --save-exact react-scripts@5.0.0

//或
yarn add --exact react-scripts@5.0.0
```

我已经在`otcd`项目开始迁移了，简单处理的话还是很快的，不过也遇到了一些问题，我记录下。

### 升级`cra`
使用我们局域网`8081`的镜像有个`fileList`包下载不了，报404，用默认镜像源会升级失败

所以升级`cra`的时候需要切换源，使用官方源和淘宝源都可以，推荐使用淘宝源，因为速度比较快

```
yarn add --exact react-scripts@5.0.0 --registry=https://registry.npm.taobao.org
```

### 开启物理缓存
`cra`默认配置的缓存模式就是物理缓存，而且关于`version`的算法也已经写好了，目前具体算法逻辑我还没看，不过从打印的version名称来看是动态生成的hash

所以这里不再需要自定义配置物理缓存和算法

### 运行遇到的问题
#### 1. 使用`query`报错
在`login.js`中使用`query`库，升级前一切正常，升级后报错没有这个包。

`query`不是顶层依赖包，而是依赖的依赖包，这种情况的出现正是我前面在`pnpm`部分所说的权限控制不做好就会出现这种问题。

牢记：“任何情况下都不要依赖于依赖的依赖”，因为实现者是随时可以替换或舍弃这个包的。

有很多种可能，不过我认为最可能的一种是：react-scripts5.0之前的版本使用了这个依赖，而5.0开始不再使用这个包，所以报错。【不过query库太基础了，是react-scripts的依赖的依赖也很有可能，不过不管是哪个依赖的依赖都不重要，重要的是开发者不应该在项目中使用依赖的依赖，要求必须使用顶层依赖！】

解决方案：安装`query`

#### 2. `eslintrc.js`报错
具体报错`Failed to load plugin 'react-hooks' declared in '.eslintrc.js': Cannot find module 'eslint-plugin-react-hooks' Require stack:`

具体原因我还没查...

解决方案：安装`eslint-plugin-react-hooks`

#### 3. 从`rootnet/dateFormat`导出`N2`异常
因为N2根本就不在`dateFormat`里，而是`format`抛出的方法。

这个错误应该和`cra`升级无关，需要考虑的是这种情况是如何出现的，以及之后如何避免类似的问题

#### 4. 图片读取异常
生成文件的相关配置应该变了，找个时间查下源码。【这个不影响服务启动，所以我先没处理】

# 统一配置【`umi4`开源方案】

# 包版本控制
在2022.1.10，前端一个著名的库`colors`被进行了恶意提交，在前端圈子里闹得沸沸扬扬的。

简单说一下事件经过，color是用于为输出文字添加颜色的npm库，在前端使用非常广泛【目前star数4.9k】

这个库本来是免费的开源库，2022.1.10，开发者大概是觉得自己天天用爱发电很不爽，所以做了一次恶意提交，搞坏了无数项目。

提交大意是这样的：
```js
for(let i=666;i<=Infinity;i++){
    //打印美国国旗字符
}
```

具体提交和相关评论可见 [colors恶意提交](https://github.com/Marak/colors.js/commit/074a0f8ed0c31c35d13d28632bd8a049ff136fb6)

我们项目也因此受到波及，因为无限循环的缘故，导致部署被卡死，大约因此耽误了我们1到2个小时的发版时间。

这个恶意提交代码的作者的确不是好东西，但这一次也暴露了包管理的弊端和问题，通过这一次的坑，我们应该意识并解决包管理不严格的情况。

常言：塞翁失马焉知非福，在没有造成大的影响下，更早意识到目前包管理的问题，对我们来说是一件好事。

目前我们很多包都是用了`^`符号自动升级小版本，这种做法问题很大，一但其中某个包出现了恶意提交，且它不像colors这样部署过程中就能发现，这种隐藏的问题代码更加致命。

我们不应当期望这是公共问题，所以有社区会有公共方法，这种将希望寄托在其他人的想法很致命，那是想的太漂亮了。

就以colors来说，我们是发版时发现的问题，但是最开始的时候我们并不知道是哪个包导致的问题，我们定位到是`dynamic-pack`【以下简称dp】包的问题。

首先一点是`dp`包本地测试是正常的，但是线上和其他人执行却遭遇了问题。

`dp`本身逻辑是正常的，为什么会出现，是依赖的colors导致的，我开发时使用的是正常版本的，而等到打包时下载的colors包却是异常的，因为异常包的当天发布的，而部署打包会下载最新包，由此产生了这个问题

解决这个问题，我是自己一步步定位出问题并进行了临时修复，老实说这个包的问题网上并没有立刻爆出来，因为当天发布的问题，反馈有所延迟，当时我查的时候网上并没有colors的问题爆出来，这里是我自己定位问题修复的。【是事实，大概两三周之后，在过年的时候，我才刷到这个瓜。。。】

我这里是想说，对于我们来说，具备自己定位和修复问题的能力很重要，我们并不一定需要自己解决问题，更希望是社区提供方案，但是有时候情况紧急，那么独立修复问题则几乎是必然的。

不过，我认为进行修复本身就不是好情况，我希望没有问题，没有问题就是最好的，既不需要定位问题，也不需要修复。

目前的包管理方式，如果不做控制自由的让包进行升级，则很可能再次出现以上问题，而情况更危险一些，直到发版这个问题才出现，那就很麻烦了，修复的成本【并非难度】是开发阶段的数倍乃至更高。

我现在的想法是：
1. 默认锁定安全的版本
2. 视情况手动对相关包进行升级
   1. 更新特性
   2. 修复缺陷

如果一个包某个版本是稳定的，我们就使用稳定版本，锁定这个版本，目前npm是没有审核，如果某个包是个人开发，那他连code review都没有，那基本说他想怎么改就怎么改，我对这种个人开发的包现在一点安全感没有，即使公司参与的包依旧可能存在问题，比如说好像是2021吧，antd-design这个包都有人埋坑，在圣诞节那一天按钮会出现雪花遮挡，这个量级的大公司，还是团队合作，这种坑都能过去，我认为我们还是不能对外部依赖包有太高期待。

针对包管理我自己是两点看法：

首先，个人需要具备独立定位和修复开源包问题的能力，不能依赖社区，因为社区并不做任何保证，没有解决方案的情况是存在的，同时社区具有延后性

比如说问题是当天出现的，我们当天需要发版，但是这个问题社区没有动静，难道我们要等社区几天出方案？这一步都是另说，能不能快速定位到出问题的包是第一步，解决还是下一步，至少等直到问题是拿到包导致的包，不然怎么查？以我们项目问题，你说部署乱码，鬼知道怎么回事？稍微进一步，你说知道是dynamic-pack库的问题，可这个库是我们内部开源库，外面查不到，别人根本看不到库代码~所以我们至少要定位到问题是dynamic-pack导致的，但到这一步你还是没办法解决问题，因为我说了，代码是正常的，逻辑是合理的，问题不在于代码本身。

比如说我使用一个数学运算库，计算1+1：
```js
math.add(1,2);
```
代码没问题，逻辑也合理，可坑比的是有人恶意修改add，里面有个无限循环的副作用，你一跑整个项目就卡死，你只看代码是没问题，因为代码是正确的，你需要自己定位到问题在哪【是哪个包的问题】，然后针对性的进行修复或者求援开源社区

这个`dp`遇到的问题也是一样，它的代码逻辑是正常的，依赖包正常的情况下执行也是正常的，然而问题就在于依赖包，它有了恶意提交。

不要认为定位哪个包有问题简单，事实上我们必须先排除`dp`本身的的代码问题，然后才会考虑依赖包的问题，其次库里有很多依赖包，是一个有问题？还是两个有问题？又或许都没有问题，遇到一起产生问题？

幸运的是，`dp`代码量不大【1000行以内】，依赖的包数量也少【10个以内】

我们再想想，如果是我们`otcd`的项目出现问题，比如说假设lodash做了恶意提交，在`get`方法里有一个无限循环的副作用，我们要付出多大的时间和人力成本才能确定问题？

如果你不知道问题是什么，那你怎么进行修复？

由此可见，修复问题可能并不是问题的关键，定位问题更加重要，我们需要先定位dp，然后定位到dp的colors包异常，到这一步才能向开源社区求助。

不过这个时候是尴尬的，一个是开源社区此时并没有相关问题【比如colors这次刚刚提交我们就踩坑了，我们就是最早踩坑的那一批】，而且我们没有那么多时间等待社区的回应，我们必须尽快解决这个问题，我们解决问题的时间单位是分钟，不是小时，或者天，发版就在此时，整个团队的运转都是一触即发的状态，我们没有太多的时间。

实际上，这里我们在社区没有得到帮助，自我修复的问题就在于还需要花时间进行测试和验证，这是不可缺少的，而如果有社区方案通常已经进行了测试，而且比较全面。

不过，事到如今，我们唯有自己解决这个问题，最终做了修复和本地后进行了提交。从问题提出到修复，整个过程大约花费了1个半小时，所幸没有产生大的影响。

总之，一旦问题出现，解决它的代价就是巨大的。不过，我们总是要考虑糟糕的情况，如果再一次出现问题，那么预期开发者具备定位问题的能力就排在了第一位，不知道问题是什么的话，别说进行修复，我们甚至无法向社区或官方求助。

一般来说，我们会得到一个好的社区方案，这也是我们期望和推荐的。不过，如果万一这个问题是头一遭出现，没有现成的方案，我们也必须具备自己进行修复的能力和魄力，当然这是极端情况，我希望不会出现。

上面说了这么多，我相信应该可以让我们了解到一旦依赖包出现问题，相比于业务代码的修复，它的难度和成本都是更高层次的，可以避免的话，当然是尽可能避免。

我们不应该让未知的变化出现在项目中，让一切变化尽在掌控，所以我第二点想法就是： 锁定版本，手动控制升级