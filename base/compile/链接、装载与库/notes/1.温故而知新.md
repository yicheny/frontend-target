[TOC]

# 硬件
计算机的关键部件：
1. CPU
   1. 数据
   2. 代码
   3. 控制
2. 内存
3. I/O控制芯片

## 北桥芯片
随着图形化设备的普通，尤其是3D游戏和多媒体的发展，使得图形设备需要跟CPU和内存大量交换数据，原本的I/O总线无法满足需求，于是为了高速交互数据的需求，专门设计了北桥芯片

## 南桥芯片
因为北桥运行速度很高，如果将低速设备，那么北桥既需要处理低速设备，又需要处理告诉设备，设计会变得很复杂。因此又设计了专门处理低速设备的南桥芯片。

## SMP与多核
Intel创始人戈登-摩尔在1965年曾预言：在未来10，每26个月CPU的运算速度会翻倍。这就是著名的摩尔定律。

实际上，这个预言并不准确，CPU的实际发展更快，基本每18个月翻一倍，而且影响周期也远远不止10年。

数十年以来，摩尔定律都被遵守着，从最开始的几十KHz到现在的4GHz

然而，目前制造CPU已经遇到了瓶颈，达到了物理上的极限，至今10年以上单核CPU的频率都被限制在4GHz左右。

CPU短期内难以翻倍式的进行频率提升了，于是人们开始考虑从另一个角度提高CPU速度，那就是增加CPU数量。

### SMP
很多年前，最常见的多CPU形式是**对称多处理器**（SMP，Symmetrical Multi-Processing）

简单将就是每个CPU在系统中的地位和功能都是一样的，是相互对称的。

理论上我们希望，增加CPU数量就可以提高运算速度，最理想的情况，增加的CPU数量和提升的速度成正比。

然而这并不可能，根本原因在于：程序并不能无限拆解为若干个不相干的子问题。

一个简单而有效的比喻：一个女人可以10个月生1个孩子，10个女人不能1个月生1个孩子。

话虽如此，多CPU依旧是非常有用的，因为实际中存在许多独立运算，比如网络服务器需要大量请求，而这些请求往往是独立的。

### MP
多处理器使用最多的就是商用处理器，对于个人电脑而言，多处理器是非常奢侈的行为，因为多处理器成本很高。

厂商为此考虑将多个处理器“打包出售”，“打包”的处理器共享昂贵的缓存部件，只保留多个核心，然后以一个处理器的外包装进行出售，售价比单核只贵一些。

这就是**多核处理器**（MP,Multi-core Processor）的基本相反。

可以认为MP就是SMP的简化版本，区别主要是在缓存部分。


# 计算机系统软件体系
一般将用于管理计算机本身的软件称为**系统软件**

系统软件可分为两部分：
1. 平台性的：操作系统内核、驱动程序、运行库、系统工具等
2. 用于程序开发的：编译器、汇编器、链接器等开发工具和工具库

计算机系统软件体系采用一种**层**的结构。

- 计算机领域有一句名言：“计算机科学领域的任何问题都可以通过 增加一个间接的中间层来解决”
- 这句话几乎概括了计算机系统软件体系结构的设计要点，整体系都是从上到下按照严格的层次结构设计的
- 每个层次之间需要相互通信，为此需要通信协议，一般称为**接口**
  - 接口的下面是这层接口的提供者，由它定义接口
  - 接口的上层是接口的使用者，使用接口实现所需要的功能
- 可以认为，除了硬件和应用程序，中间的每一层都是中间层，可以被修改或替换
  - 每个中间层都是对下层的包装和扩展
  - 中间层的存在，使得上层和下层相对独立，比如应用程序和硬件相互独立。这种设计也利于各部分之间的发展，因为不需要考虑其他部分，只需要遵守协议。

开发系统和应用程序属于相同层次，它们都使用同一个接口，即：**应用程序编程接口**（API，Application Programming Interface）

运行库使用操作系统提供的**系统调用接口**（System call Interface）

系统该调用接口在实现中往往以**软件中断**的方式提供。

# 操作系统做什么？
1. 提供抽象接口
2. 管理硬件资源

# 发挥CPU能量
## 多道程序
计算机发展早期，CPU资源是非常昂贵的，我们希望极可能的发挥CPU的力量。

在早期，CPU是只能运行一个程序，那么假设程序读写磁盘时，CPU就空闲了，这对CPU是极大的浪费。

所以人们编写了一个监控程序，如果某个程序暂时不需要使用CPU，那么就将另外等待CPU的程序启动，以更充分的利用CPU。

这种机制被称为**多道程序**（Multiprogramming）

## 分时系统
多道程序的问题在于：程序之间不分轻重缓急，有些程序对CPU响应时间要求很高【比如用户交互】，这种需要尽快分配CPU资源。

因此，在多道程序基础上进行了改良：每个程序运行一段程序后主动让出CPU给其他程序，使得一段程序内每个程序都有机会运行一小段时间。

这种改良对交互式任务帮助很大，比如点击鼠标或者按下键盘，需要的CPU资源不多，但是需要尽快被处理。

这种新的模式我们叫做**分时系统**（Time-Sharing System）

## 多任务系统
分时系统也存在一个问题：如果程序在进行一个很耗时的复杂运算，或者陷入了一个死循环，这在分时系统中无法被退出，此时整个系统被仿佛被卡死了一样。

任何一个程序的异常都有可能导致整个系统被卡死，这很难让人接受。

在分时系统之上，一个更为先进的模式出现了，它就是：**多任务系统**（Multi-tasking System）

介绍下原理：
1. 操作系统接管了所有硬件资源，并且本身运行在一个受硬件保护的级别
2. 所有的应用程序都以**进程**（Process）运行在比操作系统权限更低的的级别
3. 每个进程都有自己独立的地址空间，使得进程之间的地址空间进行隔离
4. CPU由操作系统统一进行分配，每个进程根据优先级使用CPU
5. 如果运行时间超过了一定时间，操作系统就会暂停该进程，将CPU分配给其他进程
   1. 这种CPU分配方式被称作**抢占式**（Preemptive），操作系统可以强制剥夺CPU资源分配给它目前最需要的进程
6. 操作系统分配给每个进程的时间都很短，那么CPU在多个进程间快速切换，就会造成多个进程同时运行的假象

目前几乎所有的操作系统都使用这种方式管理程序。

# 设备驱动
对应用层而言，我们希望看到统一的访问模式，操作系统将繁杂的硬件抽象成一系列概念：
1. 图像 => GDI
2. 多媒体 => DirectX
3. 磁盘 => 文件系统
4. ……

硬件的管理由操作系统的**硬件驱动（Device Deriver）程序**完成。

需要明确的是：硬件驱动并不直接为硬件开发驱动程序，它提供一系列接口和框接，对应的驱动程序由硬件厂商提供。

# 内存分配
如果一个计算机同时只运行一个程序，那么只要程序要求的内存空间不超过对物理内存的大小就可以。

但是我们知道，为了充分利用CPU，计算机会同时运行多个程序。

问题在于：如何将计算机上有限的物理内存分配给多个程序使用？

## 示例
举个例子：
假设我们内存一共是64m，A程序需要10m,B程序需要50m,C程序需要30m；并且，这个3个程序使用内存时都是直接访问物理地址。

如果独立运行程序，任意一个都可以正常运行。

假设现在我们想同时运行A、B两个程序，简单方案就是将1-10m的空间给A程序使用，11-60m的内容给B程序使用。

我们来看下这样会存在什么问题？
1. **地址空间不隔离**：所有程序都是直接访问地址的，那么一个程序想要修改另一个程序的数据是很容易的，这很容易造成bug
2. **内存使用效率低**：如果我们想要运行C，只能释放B【因为释放A也不够】，那么需要将B先写入磁盘，再从磁盘中读取C到内存；这个过程效率很低
3. **运行地址不稳定**：每次装载程序时，我们都需要在内存中分配一块足够大的空间，这个空间通常是不确定的，程序如果使用确定的物理定制进行访问，就会出现问题。

解决以上问题的思路就是加个中间层，将物理地址抽象，使用虚拟地址访问。

这么做的好处是，我们可以保证访问的物理区域和另一个程序相互隔离且不重叠。而且物理地址不再需要固定的大段区域，可以更好的对内存进行利用。

## 分段
最开始使用的技术是**分段**，即：把一段与程序所需要的 内存空间大小 的虚拟空间映射到 物理地址空间。

以A程序举例，我们需要10m空间内存，可以使用虚拟地址0-10m，但是虚拟地址映射的物理地址没有限制，只需要满足大小为10m即可，比如11-20m区域，或者12-21m区域，差不多这个意思。

分段可以解决两个问题：
1. 地址空间不隔离：现在有虚拟地址这一层，可以限制程序的访问地址范围
2. 运行地址不稳定：我们绑定的是虚拟地址，它是抽象的，我们只需要在物理内存中划分一块所需大小的空间即可。

分段没有解决内存使用效率的问题，因为它还是以程序为单位进行内存分配的。

根据**程序的局部性原理**：一个程序在运行的某段时间内，它只会频繁的用到一小部分程序，就是说，我们加载程序是不需要一次性分配所有空间，我们可以进行更细粒度的内存分配，以提高内存的使用率，这种技术叫做**分页**（Paging）

## 分页
分页就是将地址空间分成固定大小的页，页的大小：
1. 由硬件决定
2. 硬件支持多种页大小，由操作系统决定

总的来说，从系统角度看，页的大小是固定不变的。【目前基本都是4kb每页】

我们通过一个例子来了解页的分配模式。

我们现在有32k的内存，4k一页，我们有8页

首先我们有一个程序A，我们分配进程执行程序A，这个进程我们标记为ProcessA，

程序A一共需要使用20K内存，共需要5页内存空间，如图：

![](https://pic.imgdb.cn/item/61f0fb2c2ab3f51d9171e6be.jpg)

但是开始执行ProcessA时，我们只使用了其中3页。

![](https://pic.imgdb.cn/item/61f0fb722ab3f51d9172252e.jpg)

我们首先加载`VP2`的数据，我们看下`VP2`在物理内存中的映射：

![](https://pic.imgdb.cn/item/61f0fc112ab3f51d9172af38.jpg)

`VP2`在物理内存中，这很好。假设我们现在想要加载VP4和VP7，这部分数据并不在物理内存中，那么读取会发生**页错误**（Page Fault）。

此时操作系统会接管进程，将数据从磁盘中读取到内存中，然后再通过虚拟地址读取。

![](https://pic.imgdb.cn/item/61f0fda32ab3f51d91741cf8.jpg)

我们进一步考虑，如果现在有程序B的进程ProcessB需要执行，它需要占据6页，但是现在剩余空间只有5页。

![](https://pic.imgdb.cn/item/61f0ffd12ab3f51d9175ff5b.jpg)

如果是分段场景，则需要将ProcessA占用的内存全部释放，然后整个装载ProcessB。

但是分页场景下不需要释放整个ProcessA占用的内存使用，只需要释放足以支持ProcessB运行的内存页即可。

![](https://pic.imgdb.cn/item/61f100332ab3f51d9176518d.jpg)

可以看到，利用分页机制，可以更有效的使用内存，提高运行效率。

分页另一个目的是**保护**，我们针对每个页设置权限，比如设置VP1可写，VP2只读这样。

利用权限控制机制，操作系统可以实现对自己和进程的保护。

### MMU
虚拟内存需要依靠硬件支持，几乎所有硬件都采用了一种叫做**MMU**（Memory Management Unit）的部件来进行映射，如图：

![](https://pic.imgdb.cn/item/61f107872ab3f51d917c138f.jpg)


# 线程
## 什么是线程？
线程（Thread），有时被称为轻量级进程（LWP,Lightweight Process），是程序执行流的最小单元。

一个标准的线程组成：
1. 线程ID
2. 当前指令指针（PC）
3. 寄存器组合
4. 堆栈

一个进程由多个线程组成，各个线程之间共享程序的内存空间及一些进程级资源。

经典的线程与进程关系如图：

![](https://pic.imgdb.cn/item/61f10d902ab3f51d91842edd.jpg)

使用多线程的原因：
1. 某个操作可能会陷入长时间等待，比如网络响应，利用线程可以利用这段等待时间
2. 某个操作可能会消耗大量时间，利用线程可以保证交互的正常
3. 程序逻辑要求并发操作，比如同时下载多个文件
4. 多线程可以更充分的发挥算力【比如拆解子任务同时进行】
5. 相对于多进程，多线程可以进行数据共享，或者说数据共享效率要高得多

## 共享数据和私有数据
线程可以共享进程数据，同时它也有自己的私有空间，包括以下：
1. 栈
2. 线程局部存储（Thread Local Storage,TLS），通常容量很有限
3. 寄存器，寄存器是执行流的基本数据，所有必然是私有的

我们从程序角度观看线程中的公有和私有数据
1. 私有
   1. 局部变量
   2. 函数参数
   3. TLS数据
2. 共享
   1. 全局变量
   2. 堆数据
   3. 函数的静态变量
   4. 程序代码
   5. 打开的文件

## 线程调度
1. 线程是并发执行的
2. 当线程数小于等于CPU数，是真正的并发
3. 当线程数大于CPU数，则必有CPU同时执行复数以上线程，此时CPU会采取**线程调度**（Thread Schedule）
   1. 简单来说，就是此时的并发是模拟的
   2. 多线程程序会轮流执行，每次执行一小段时间
   3. 因为人类感受不到这么微小的时间变化，所以感觉是“同时”执行

线程调度至少有3种状态：
1. 运行（Running）
2. 就绪（Ready）：可以立即运行，但是CPU被占用
3. 等待（Waiting）：等待某一事件（通常是I/O）发生，无法执行

状态关系图如下：

![](https://pic.imgdb.cn/item/61f117652ab3f51d918c56a6.jpg)

### 优先级
线程调度自多任务操作系统以来就不断被提出新的算法和方案，目前主流方案都带有优先级调度和轮转法的痕迹。

- **轮转法**：各个线程轮流执行一小段时间
- **优先级调度**：决定线程按照什么顺序轮流执行。简单来说，每个线程都有优先级，按优先级从高到底执行

除了手动调整线程等级，调度系统也会智能的进行调整，一些自动调整优先级的机制：
- 一般来说，IO密集型线程相比CPU密集型线程会被赋予更高的优先级。【这个过程是调度系统根据线程表现逐步调整的】
- 为了避免饿死现象，调度系统会逐步提升那些等待了较长时间的线程

### 不可抢占线程
在早期系统中，存在不可抢占线程，此时线程需要主动退出进行就绪状态，不可抢占线程退出有两种情况：
1. 等待
2. 主动放弃时间片

# 名词
- 物理页（PP,Physical Page）：物理内存中的页
- 虚拟页（VP,Virtual Page）：虚拟空间的页
- 磁盘页（DP,Disk Page）：磁盘中的页
- 线程调度：再处理器上切换不同线程的行为
- 时间片（Time Slice）：线程调度时，线程可执行的一段时间
- IO密集型线程（IO Bound Thread）：频繁等待的线程
- CPU密集型线程（CPU Bound Thread）：很少等待的线程
- 饿死（Starvation）：说一个线程被饿死，就是说它优先级较低，总是被高优先级的线程抢占执行，它时钟无法得到执行。
- 抢占（Preemption）：线程用尽时间片之后被强制剥夺执行权利，进入就绪状态这个过程叫做抢占